# Project 4: Data Lake


### Introduction 
Sparkify is an starup and the analytics team is particularly interested in understanding what songs users are listening to on their new music streaming app. Currently, their data resides in a directory of **JSON logs** on user activity on the app, as well as a directory with JSON metadata on the songs in their app. However, this cannot provid an easy way to query the data.

The purpose of this project is to insert the current Data Warehouse into the Big Data reality.
In this project, you'll apply what you've learned on Spark and data lakes to build an ETL pipeline for a data lake hosted on S3. To complete the project, you will need to load data from S3, process the data into analytics tables using Spark, and load them back into S3. You'll deploy this Spark process on a cluster using AWS.

## Data Sources

Data resides in two directories that contain files in JSON format:

1. **s3a://udacity-dend/song_data** : Contains metadata about a song and the artist of that song;
2. **s3a://udacity-dend/log_data** : Consists of log files generated by the streaming app based on the songs in the dataset above;



##  How it works

The project template includes two files:

-   `etl.py`  reads data from S3, processes that data using Spark, and writes them back to S3
-   `dl.cfg`contains the AWS credentials (Hidden)
 